{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Numerical Gradient Checking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We would highly recommend looking at `neural_networks.grad_check.check_gradients` and making sure you understand how numerical gradient checking is being carried out. This function is used in the notebook to check the gradients of the neural network layers you write. Make sure to check the gradient of a layer after finishing its implementation.\n",
    "\n",
    "The function returns the relative error of the numerical gradient (approximated using finite differences) with respect to the analytical gradient (computed via backpropagation). Correct implementations should get very small errors, usually less than `1e-8` for 64-bit float matrices (the default)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import numpy as np\n",
    "from neural_networks.utils import check_gradients\n",
    "from neural_networks.layers import FullyConnected, Conv2D, Pool2D\n",
    "from neural_networks.activations import Linear, Sigmoid, TanH, ReLU, SoftMax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Checks for Activation Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.random.randn(2, 3)\n",
    "dLdY = np.random.randn(2, 3)\n",
    "\n",
    "# initialize a fully connected layer\n",
    "# and perform a forward and backward pass\n",
    "linear_activation = Linear()\n",
    "_ = linear_activation.forward(X)\n",
    "grad = linear_activation.backward(X, dLdY)\n",
    "\n",
    "# check the gradients w.r.t. each parameter\n",
    "print(\n",
    "    f\"Relative error for linear activation:\",\n",
    "    check_gradients(\n",
    "        fn=linear_activation.forward,  # the function we are checking\n",
    "        grad=grad,  # the analytically computed gradient\n",
    "        x=X,        # the variable w.r.t. which we are taking the gradient\n",
    "        dLdf=dLdY,  # gradient at previous layer\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sigmoid Activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.random.randn(2, 3)\n",
    "dLdY = np.random.randn(2, 3)\n",
    "\n",
    "# initialize a fully connected layer\n",
    "# and perform a forward and backward pass\n",
    "sigmoid_activation = Sigmoid()\n",
    "_ = sigmoid_activation.forward(X)\n",
    "grad = sigmoid_activation.backward(X, dLdY)\n",
    "\n",
    "# check the gradients w.r.t. each parameter\n",
    "print(\n",
    "    f\"Relative error for sigmoid activation:\",\n",
    "    check_gradients(\n",
    "        fn=sigmoid_activation.forward,  # the function we are checking\n",
    "        grad=grad,  # the analytically computed gradient\n",
    "        x=X,        # the variable w.r.t. which we are taking the gradient\n",
    "        dLdf=dLdY,  # gradient at previous layer\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tanh Activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.random.randn(2, 3)\n",
    "dLdY = np.random.randn(2, 3)\n",
    "\n",
    "# initialize a fully connected layer\n",
    "# and perform a forward and backward pass\n",
    "tanh_activation = TanH()\n",
    "_ = tanh_activation.forward(X)\n",
    "grad = tanh_activation.backward(X, dLdY)\n",
    "\n",
    "# check the gradients w.r.t. each parameter\n",
    "print(\n",
    "    f\"Relative error for tanh activation:\",\n",
    "    check_gradients(\n",
    "        fn=tanh_activation.forward,  # the function we are checking\n",
    "        grad=grad,  # the analytically computed gradient\n",
    "        x=X,        # the variable w.r.t. which we are taking the gradient\n",
    "        dLdf=dLdY,  # gradient at previous layer\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ReLU Activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.random.randn(2, 3)\n",
    "dLdY = np.random.randn(2, 3)\n",
    "\n",
    "# initialize a fully connected layer\n",
    "# and perform a forward and backward pass\n",
    "relu_activation = ReLU()\n",
    "out = relu_activation.forward(X)\n",
    "grad = relu_activation.backward(X, dLdY)\n",
    "\n",
    "# check the gradients w.r.t. each parameter\n",
    "print(\n",
    "    f\"Relative error for relu activation:\",\n",
    "    check_gradients(\n",
    "        fn=relu_activation.forward,  # the function we are checking\n",
    "        grad=grad,  # the analytically computed gradient\n",
    "        x=X,        # the variable w.r.t. which we are taking the gradient\n",
    "        dLdf=dLdY,  # gradient at previous layer\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Softmax Activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.random.randn(2, 3)\n",
    "dLdY = np.random.randn(2, 3)\n",
    "\n",
    "# initialize a fully connected layer\n",
    "# and perform a forward and backward pass\n",
    "softmax_activation = SoftMax()\n",
    "_ = softmax_activation.forward(X)\n",
    "grad = softmax_activation.backward(X, dLdY)\n",
    "\n",
    "# check the gradients w.r.t. each parameter\n",
    "print(\n",
    "    f\"Relative error for softmax activation:\",\n",
    "    check_gradients(\n",
    "        fn=softmax_activation.forward,  # the function we are checking\n",
    "        grad=grad,  # the analytically computed gradient\n",
    "        x=X,        # the variable w.r.t. which we are taking the gradient\n",
    "        dLdf=dLdY,  # gradient at previous layer\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Checks for Full Layers (Linear Activations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fully Connected Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.random.randn(2, 3)\n",
    "dLdY = np.random.randn(2, 4)\n",
    "\n",
    "# initialize a fully connected layer\n",
    "# and perform a forward and backward pass\n",
    "fc_layer = FullyConnected(n_out=4, activation=\"linear\")\n",
    "_ = fc_layer.forward(X)\n",
    "_ = fc_layer.backward(dLdY)\n",
    "\n",
    "# check the gradients w.r.t. each parameter\n",
    "for param in fc_layer.parameters:\n",
    "    print(\n",
    "        f\"Relative error for {param}:\",\n",
    "        check_gradients(\n",
    "            fn=fc_layer.forward_with_param(param, X),  # the function we are checking\n",
    "            grad=fc_layer.gradients[param],  # the analytically computed gradient\n",
    "            x=fc_layer.parameters[param],  # the variable w.r.t. which we are taking the gradient\n",
    "            dLdf=dLdY,                     # gradient at previous layer\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conv Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 2, 2, 2)\n",
      "(2, 2, 2, 2)\n"
     ]
    }
   ],
   "source": [
    "testX = np.array(\n",
    "    [ \n",
    "        [#B0\n",
    "            [ #h0\n",
    "                [ #w0\n",
    "                    2,0\n",
    "                ],\n",
    "                [ #w1\n",
    "                    2,0\n",
    "                ]\n",
    "            ],\n",
    "            [ #h1\n",
    "                [ #w0\n",
    "                    2,0\n",
    "                ],\n",
    "                [ #w1\n",
    "                    2,0\n",
    "                ]\n",
    "            ],\n",
    "        ],\n",
    "\n",
    "        [#B1\n",
    "            [ #h0\n",
    "                [ #w0\n",
    "                    0,1\n",
    "                ],\n",
    "                [ #w1\n",
    "                    0,1\n",
    "                ]\n",
    "            ],\n",
    "            [ #h1\n",
    "                [ #w0\n",
    "                    0,1\n",
    "                ],\n",
    "                [ #w1\n",
    "                    0,1\n",
    "                ]\n",
    "            ],\n",
    "        ],   \n",
    "\n",
    "    ]\n",
    ")\n",
    "\n",
    "filter = np.array(\n",
    "    [ \n",
    "       [#h0\n",
    "           [#w0\n",
    "               [#cin0\n",
    "                   1, 0\n",
    "               ],\n",
    "               [#cin1\n",
    "                   2, 0\n",
    "               ]\n",
    "           ],\n",
    "           [#w1\n",
    "               [#cin0\n",
    "                   1, 0\n",
    "               ],\n",
    "               [#cin1\n",
    "                   2, 0\n",
    "               ]\n",
    "           ]\n",
    "       ],\n",
    "\n",
    "       [#h1\n",
    "           [#w0\n",
    "               [#cin0\n",
    "                   1, 0\n",
    "               ],\n",
    "               [#cin1\n",
    "                   2, 0\n",
    "               ]\n",
    "           ],\n",
    "           [#w1\n",
    "               [#cin0\n",
    "                   1, 0\n",
    "               ],\n",
    "               [#cin1\n",
    "                   2, 1\n",
    "               ]\n",
    "           ]\n",
    "       ]\n",
    "\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(testX.shape)\n",
    "# print(testX)\n",
    "print(filter.shape)\n",
    "# print(filter)\n",
    "Z = np.zeros((2, 3, 3, 2))\n",
    "b = np.array([[1,1]])\n",
    "# print(b.shape)\n",
    "out = np.einsum('...hwc, hwcd -> ...d', testX, filter)\n",
    "\n",
    "# print(out)\n",
    "Z[:, 0, 0, :] = out + b\n",
    "# print(Z)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[4 1]\n",
      "  [4 1]]\n",
      "\n",
      " [[4 1]\n",
      "  [4 1]]]\n"
     ]
    }
   ],
   "source": [
    "multiply = np.array([2, 1])\n",
    "multiply.shape\n",
    "\n",
    "out = np.einsum('bhwc, b -> hwc', testX, multiply)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Relative error for W: 9.421460426047668e-11\n",
      "Relative error for b: 5.067190559758689e-11\n"
     ]
    }
   ],
   "source": [
    "X = np.random.randn(2, 6, 6, 7)\n",
    "dLdY = np.random.randn(2, 6, 6, 4)\n",
    "\n",
    "# initialize a fully connected layer\n",
    "# and perform a forward and backward pass\n",
    "conv_layer = Conv2D(\n",
    "    n_out=4,\n",
    "    kernel_shape=(3, 3),\n",
    "    activation=\"linear\",\n",
    "    weight_init=\"uniform\",\n",
    "    pad=\"same\",\n",
    ")\n",
    "_ = conv_layer.forward(X)\n",
    "_ = conv_layer.backward(dLdY)\n",
    "\n",
    "# check the gradients w.r.t. each parameter\n",
    "for param in conv_layer.parameters:\n",
    "    print(\n",
    "        f\"Relative error for {param}:\",\n",
    "        check_gradients(\n",
    "            fn=conv_layer.forward_with_param(param, X),  # the function we are checking\n",
    "            grad=conv_layer.gradients[param],  # the analytically computed gradient\n",
    "            x=conv_layer.parameters[param],  # the variable w.r.t. which we are taking the gradient\n",
    "            dLdf=dLdY,                     # gradient at previous layer\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "testX = np.array(\n",
    "    [ \n",
    "        [#B0\n",
    "            [ #h0\n",
    "                [ #w0\n",
    "                    3,0\n",
    "                ],\n",
    "                [ #w1\n",
    "                    2,1\n",
    "                ]\n",
    "            ],\n",
    "            [ #h1\n",
    "                [ #w0\n",
    "                    2,0\n",
    "                ],\n",
    "                [ #w1\n",
    "                    2,0\n",
    "                ]\n",
    "            ],\n",
    "        ],\n",
    "\n",
    "        [#B1\n",
    "            [ #h0\n",
    "                [ #w0\n",
    "                    0,1\n",
    "                ],\n",
    "                [ #w1\n",
    "                    0,1\n",
    "                ]\n",
    "            ],\n",
    "            [ #h1\n",
    "                [ #w0\n",
    "                    1,1\n",
    "                ],\n",
    "                [ #w1\n",
    "                    0,3\n",
    "                ]\n",
    "            ],\n",
    "        ],   \n",
    "\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 2]\n",
      "[[0 0]\n",
      " [0 1]]\n",
      "[0 1]\n",
      "[[[[3 0]\n",
      "   [2 1]]\n",
      "\n",
      "  [[2 0]\n",
      "   [2 0]]]\n",
      "\n",
      "\n",
      " [[[0 1]\n",
      "   [0 1]]\n",
      "\n",
      "  [[1 1]\n",
      "   [0 3]]]]\n",
      "[[3 2]\n",
      " [0 1]]\n"
     ]
    }
   ],
   "source": [
    "flat = testX[:, 0:2, 0:2, 0].flatten()\n",
    "argmax = np.argmax(np.split(flat, 2), axis = 1)\n",
    "print(argmax)\n",
    "x = np.unravel_index(argmax, (2,2))\n",
    "x = np.array(x)\n",
    "print(np.flip(x, axis = 0))\n",
    "x = np.flip(x, axis = 0)\n",
    "print(np.array(x)[:,1])\n",
    "\n",
    "print(testX)\n",
    "print(testX[:, x[:,1], x[:,0], 0])\n",
    "# testX[:, x[:,0], x[:,1], 0] = 3\n",
    "\n",
    "# np.argmax(testX, axis = 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "indices:  [[36 33]\n",
      " [36 33]\n",
      " [36 33]\n",
      " [36 33]\n",
      " [36 33]\n",
      " [36 33]\n",
      " [36 33]]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "unravel_index() missing required argument 'shape' (pos 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[40], line 11\u001b[0m\n\u001b[0;32m      6\u001b[0m poolLayer \u001b[38;5;241m=\u001b[39m Pool2D(\n\u001b[0;32m      7\u001b[0m     kernel_shape\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m3\u001b[39m),\n\u001b[0;32m      8\u001b[0m     pad\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msame\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m      9\u001b[0m )\n\u001b[0;32m     10\u001b[0m _ \u001b[38;5;241m=\u001b[39m poolLayer\u001b[38;5;241m.\u001b[39mforward(X)\n\u001b[1;32m---> 11\u001b[0m _ \u001b[38;5;241m=\u001b[39m poolLayer\u001b[38;5;241m.\u001b[39mbackward(dLdY)\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# check the gradients w.r.t. each parameter\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m param \u001b[38;5;129;01min\u001b[39;00m conv_layer\u001b[38;5;241m.\u001b[39mparameters:\n",
      "File \u001b[1;32mf:\\CS189-Projects\\hw6\\hw6_release\\code\\neural_networks\\layers.py:544\u001b[0m, in \u001b[0;36mPool2D.backward\u001b[1;34m(self, dLdY)\u001b[0m\n\u001b[0;32m    542\u001b[0m     xWindow \u001b[38;5;241m=\u001b[39m xPad[:, hStart:hEnd, wStart:wEnd, c]\n\u001b[0;32m    543\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 544\u001b[0m     np\u001b[38;5;241m.\u001b[39munravel_index(indices[hIdx, wIdx, c, :], )\n\u001b[0;32m    545\u001b[0m     \u001b[38;5;28mprint\u001b[39m(mask\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m    546\u001b[0m     dX[:, hStart:hEnd, wStart:wEnd, c] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmultiply(mask, dLdY[:, hIdx, wIdx, c])\n",
      "File \u001b[1;32m<__array_function__ internals>:200\u001b[0m, in \u001b[0;36munravel_index\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: unravel_index() missing required argument 'shape' (pos 2)"
     ]
    }
   ],
   "source": [
    "X = np.random.randn(2, 5, 6, 7)\n",
    "dLdY = np.random.randn(2, 5, 6, 4)\n",
    "\n",
    "# initialize a fully connected layer\n",
    "# and perform a forward and backward pass\n",
    "poolLayer = Pool2D(\n",
    "    kernel_shape=(3, 3),\n",
    "    pad=\"same\",\n",
    ")\n",
    "_ = poolLayer.forward(X)\n",
    "_ = poolLayer.backward(dLdY)\n",
    "\n",
    "# check the gradients w.r.t. each parameter\n",
    "for param in conv_layer.parameters:\n",
    "    print(\n",
    "        f\"Relative error for {param}:\",\n",
    "        check_gradients(\n",
    "            fn=poolLayer.forward_with_param(param, X),  # the function we are checking\n",
    "            grad=poolLayer.gradients[param],  # the analytically computed gradient\n",
    "            x=poolLayer.parameters[param],  # the variable w.r.t. which we are taking the gradient\n",
    "            dLdf=dLdY,                     # gradient at previous layer\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from neural_networks.losses import CrossEntropy\n",
    "\n",
    "num_pts = 5\n",
    "num_classes = 6\n",
    "\n",
    "# one-hot encoded y\n",
    "y_idxs = np.random.randint(0, num_classes, (num_pts,))\n",
    "y = np.zeros((num_pts, num_classes))\n",
    "y[range(num_pts), y_idxs] = 1\n",
    "\n",
    "# normalized predictions\n",
    "scores = np.random.uniform(0, 1, size=(num_pts, num_classes))\n",
    "y_hat = scores / scores.sum(axis=1, keepdims=True)\n",
    "\n",
    "cross_entropy_loss = CrossEntropy(\"cross_entropy\")\n",
    "\n",
    "def forward_fn(Y, Y_hat):    \n",
    "    def inner_forward(Y_hat):\n",
    "        return cross_entropy_loss.forward(Y, Y_hat)\n",
    "    return inner_forward\n",
    "\n",
    "loss = cross_entropy_loss.forward(y, y_hat)\n",
    "grad = cross_entropy_loss.backward(y, y_hat)\n",
    "\n",
    "print(\n",
    "    f\"Relative error for cross entropy loss:\",\n",
    "    check_gradients(\n",
    "        fn=forward_fn(y, y_hat),  # the function we are checking\n",
    "        grad=grad,  # the analytically computed gradient\n",
    "        x=y_hat,        # the variable w.r.t. which we are taking the gradient\n",
    "        dLdf=1,  # gradient at previous layer\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = np.random.rand(5, 5)\n",
    "B = np.random.rand(5, 5)\n",
    "#Question 1\n",
    "print(\"Einsum Trace: \", np.einsum('ii -> ', A))\n",
    "print(\"Np Trace: \", np.trace(A))\n",
    "\n",
    "#Question 2\n",
    "print(\"Einsum Matmul: \", np.linalg.norm(np.einsum('ij, jk -> ik', A, B)))\n",
    "print(\"Np Matmul: \", np.linalg.norm(A@B))\n",
    "\n",
    "\n",
    "A = np.random.rand(3, 4, 5)\n",
    "B = np.random.rand(3, 5, 6)\n",
    "#Question 3\n",
    "print(\"Einsum batch matmul: \", np.linalg.norm(np.einsum('kij, kjl -> kil', A, B)))\n",
    "\n",
    "result = np.empty((3, 4, 6))\n",
    "for k in range(3):\n",
    "    # print(A[k] @ B[k])\n",
    "    result[k] = A[k] @ B[k]\n",
    "print(\"Np batch matmul: \", np.linalg.norm(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
