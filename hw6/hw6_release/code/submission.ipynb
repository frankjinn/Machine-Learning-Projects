{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activation Function Implementations:\n",
    "\n",
    "Implementation of `activations.Linear`:\n",
    "\n",
    "```python\n",
    "class Linear(Activation):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, Z: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Forward pass for f(z) = z.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        Z  input pre-activations (any shape)\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        f(z) as described above applied elementwise to `Z`\n",
    "        \"\"\"\n",
    "        return Z\n",
    "\n",
    "    def backward(self, Z: np.ndarray, dY: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Backward pass for f(z) = z.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        Z   input to `forward` method\n",
    "        dY  gradient of loss w.r.t. the output of this layer\n",
    "            same shape as `Z`\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        gradient of loss w.r.t. input of this layer\n",
    "        \"\"\"\n",
    "        return dY\n",
    "\n",
    "```\n",
    "\n",
    "Implementation of `activations.Sigmoid`:\n",
    "\n",
    "```python\n",
    "class Sigmoid(Activation):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, Z: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Forward pass for sigmoid function:\n",
    "        f(z) = 1 / (1 + exp(-z))\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        Z  input pre-activations (any shape)\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        f(z) as described above applied elementwise to `Z`\n",
    "        \"\"\"\n",
    "        ### YOUR CODE HERE ###\n",
    "        return ...\n",
    "\n",
    "    def backward(self, Z: np.ndarray, dY: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Backward pass for sigmoid.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        Z   input to `forward` method\n",
    "        dY  gradient of loss w.r.t. the output of this layer\n",
    "            same shape as `Z`\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        gradient of loss w.r.t. input of this layer\n",
    "        \"\"\"\n",
    "        ### YOUR CODE HERE ###\n",
    "        return ...\n",
    "\n",
    "```\n",
    "\n",
    "Implementation of `activations.ReLU`:\n",
    "\n",
    "```python\n",
    "class ReLU(Activation):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, Z: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Forward pass for relu activation:\n",
    "        f(z) = z if z >= 0\n",
    "               0 otherwise\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        Z  input pre-activations (any shape)\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        f(z) as described above applied elementwise to `Z`\n",
    "        \"\"\"\n",
    "        return np.maximum(0, Z)\n",
    "\n",
    "    def backward(self, Z: np.ndarray, dY: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Backward pass for relu activation.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        Z   input to `forward` method\n",
    "        dY  gradient of loss w.r.t. the output of this layer\n",
    "            same shape as `Z`\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        gradient of loss w.r.t. input of this layer\n",
    "        \"\"\"\n",
    "        ### YOUR CODE HERE ###\n",
    "        \n",
    "        return np.where(Z < 0, 0, dY)\n",
    "\n",
    "```\n",
    "\n",
    "Implementation of `activations.SoftMax`:\n",
    "\n",
    "```python\n",
    "class SoftMax(Activation):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, Z: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Forward pass for softmax activation.\n",
    "        Hint: The naive implementation might not be numerically stable.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        Z  input pre-activations (any shape)\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        f(z) as described above applied elementwise to `Z`\n",
    "        \"\"\"\n",
    "        ### YOUR CODE HERE ###\n",
    "        m = np.max(Z, axis=1, keepdims=True)\n",
    "        expShifted = np.exp(Z - m)\n",
    "        softmaxOutput = expShifted/np.sum(expShifted, axis=1, keepdims=True)\n",
    "\n",
    "        return softmaxOutput\n",
    "\n",
    "    def backward(self, Z: np.ndarray, dY: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Backward pass for softmax activation.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        Z   input to `forward` method\n",
    "        dY  gradient of loss w.r.t. the output of this layer\n",
    "            same shape as `Z`\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        gradient of loss w.r.t. input of this layer\n",
    "        \"\"\"\n",
    "        ### YOUR CODE HERE ###\n",
    "        # print(\"Z: \", Z.shape)\n",
    "        # print(\"dy\", dY.shape)\n",
    "        \n",
    "        dLdZ = np.empty(Z.shape)\n",
    "        softmaxZ = self.forward(Z)\n",
    "\n",
    "        for idx in range(0,softmaxZ.shape[0]):\n",
    "            # print(softmaxZ[idx])\n",
    "            # print(softmaxZ[idx, :][:, None])\n",
    "            currPoint = softmaxZ[idx, :][:, None]\n",
    "            currdY = dY[idx,:][:, None]\n",
    "            jacobian = -currPoint @ currPoint.T\n",
    "            np.fill_diagonal(jacobian, np.array([softmax * (1 - softmax) for softmax in currPoint]))\n",
    "\n",
    "            dLdZ[idx, :][:, None] = jacobian @ currdY\n",
    "        \n",
    "        return dLdZ\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "### Layer Implementations:\n",
    "\n",
    "Implementation of `layers.FullyConnected`:\n",
    "\n",
    "```python\n",
    "class FullyConnected(Layer):\n",
    "    \"\"\"A fully-connected layer multiplies its input by a weight matrix, adds\n",
    "    a bias, and then applies an activation function.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, n_out: int, activation: str, weight_init=\"xavier_uniform\"\n",
    "    ) -> None:\n",
    "\n",
    "        super().__init__()\n",
    "        self.n_in = None\n",
    "        self.n_out = n_out\n",
    "        self.activation = initialize_activation(activation)\n",
    "\n",
    "        # instantiate the weight initializer\n",
    "        self.init_weights = initialize_weights(weight_init, activation=activation)\n",
    "\n",
    "    def _init_parameters(self, X_shape: Tuple[int, int]) -> None:\n",
    "        \"\"\"Initialize all layer parameters (weights, biases).\"\"\"\n",
    "        self.n_in = X_shape[1]\n",
    "\n",
    "        ### BEGIN YOUR CODE ###\n",
    "\n",
    "        W = self.init_weights((self.n_in, self.n_out))\n",
    "        b = np.zeros((1, self.n_out))\n",
    "\n",
    "        self.parameters = OrderedDict({\"W\": W, \"b\": b}) # DO NOT CHANGE THE KEYS\n",
    "        self.cache: OrderedDict = {\"out\":0, \"X\":0}  # cache for backprop \"W\":0, \"X\":0, \"b\":0, \n",
    "        self.gradients: OrderedDict = {\"W\":np.zeros_like(W), \"b\":np.zeros_like(b)}  # parameter gradients initialized to zero\n",
    "                                           # MUST HAVE THE SAME KEYS AS `self.parameters`\n",
    "\n",
    "        ### END YOUR CODE ###\n",
    "\n",
    "    def forward(self, X: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Forward pass: multiply by a weight matrix, add a bias, apply activation.\n",
    "        Also, store all necessary intermediate results in the `cache` dictionary\n",
    "        to be able to compute the backward pass.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X  input matrix of shape (batch_size, input_dim)\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        a matrix of shape (batch_size, output_dim)\n",
    "        \"\"\"\n",
    "        # initialize layer parameters if they have not been initialized\n",
    "        if self.n_in is None:\n",
    "            self._init_parameters(X.shape)\n",
    "\n",
    "        ### BEGIN YOUR CODE ###\n",
    "        \n",
    "        # perform an affine transformation and activation\n",
    "        Z = X@self.parameters['W'] + self.parameters['b']\n",
    "        out = self.activation(Z)\n",
    "        \n",
    "        # store information necessary for backprop in `self.cache`\n",
    "        self.cache[\"Z\"] = Z\n",
    "        self.cache[\"X\"] = X\n",
    "        ### END YOUR CODE ###\n",
    "\n",
    "        return out\n",
    "\n",
    "    def backward(self, dLdOut: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Backward pass for fully connected layer.\n",
    "        Compute the gradients of the loss with respect to:\n",
    "            1. the weights of this layer (mutate the `gradients` dictionary)\n",
    "            2. the bias of this layer (mutate the `gradients` dictionary)\n",
    "            3. the input of this layer (return this)\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        dLdOut  gradient of the loss with respect to the output of this layer\n",
    "              shape (batch_size, output_dim)\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        gradient of the loss with respect to the input of this layer\n",
    "        shape (batch_size, input_dim)\n",
    "        \"\"\"\n",
    "        ### BEGIN YOUR CODE ###\n",
    "        \n",
    "        # unpack the cache\n",
    "        Z = self.cache[\"Z\"] #output of layer\n",
    "        X = self.cache[\"X\"] #input of layer\n",
    "        W = self.parameters[\"W\"]\n",
    "        dLdZ = self.activation.backward(Z, dLdOut) #Loss with respect to param before activation\n",
    "\n",
    "        # compute the gradients of the loss w.r.t. all parameters as well as the\n",
    "        # input of the layer\n",
    "        dLdW = X.T @ (dLdZ)\n",
    "        dLdb = dLdZ.sum(axis = 0, keepdims = True)\n",
    "        dLdX = dLdZ @ W.T\n",
    "\n",
    "        # store the gradients in `self.gradients`\n",
    "        # the gradient for self.parameters[\"W\"] should be stored in\n",
    "        # self.gradients[\"W\"], etc.\n",
    "\n",
    "        self.gradients[\"W\"] = dLdW\n",
    "        self.gradients[\"b\"] = dLdb\n",
    "\n",
    "        ### END YOUR CODE ###\n",
    "\n",
    "        return dLdX #Originally dX\n",
    "\n",
    "```\n",
    "\n",
    "Implementation of `layers.Pool2D`:\n",
    "\n",
    "```python\n",
    "class Pool2D(Layer):\n",
    "    \"\"\"Pooling layer, implements max and average pooling.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        kernel_shape: Tuple[int, int],\n",
    "        mode: str = \"max\",\n",
    "        stride: int = 1,\n",
    "        pad: Union[int, Literal[\"same\"], Literal[\"valid\"]] = 0,\n",
    "    ) -> None:\n",
    "\n",
    "        if type(kernel_shape) == int:\n",
    "            kernel_shape = (kernel_shape, kernel_shape)\n",
    "\n",
    "        self.kernel_shape = kernel_shape\n",
    "        self.stride = stride\n",
    "\n",
    "        if pad == \"same\":\n",
    "            self.pad = ((kernel_shape[0] - 1) // 2, (kernel_shape[1] - 1) // 2)\n",
    "        elif pad == \"valid\":\n",
    "            self.pad = (0, 0)\n",
    "        elif isinstance(pad, int):\n",
    "            self.pad = (pad, pad)\n",
    "        else:\n",
    "            raise ValueError(\"Invalid Pad mode found in self.pad.\")\n",
    "\n",
    "        self.mode = mode\n",
    "\n",
    "        if mode == \"max\":\n",
    "            self.pool_fn = np.max\n",
    "            self.arg_pool_fn = np.argmax\n",
    "        elif mode == \"average\":\n",
    "            self.pool_fn = np.mean\n",
    "\n",
    "        self.cache = {\n",
    "            \"out_rows\": [],\n",
    "            \"out_cols\": [],\n",
    "            \"X_pad\": [],\n",
    "            \"p\": [],\n",
    "            \"pool_shape\": [],\n",
    "        }\n",
    "        self.parameters = {}\n",
    "        self.gradients = {}\n",
    "\n",
    "    def forward(self, X: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Forward pass: use the pooling function to aggregate local information\n",
    "        in the input. This layer typically reduces the spatial dimensionality of\n",
    "        the input while keeping the number of feature maps the same.\n",
    "\n",
    "        As with all other layers, please make sure to cache the appropriate\n",
    "        information for the backward pass.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X  input array of shape (batch_size, in_rows, in_cols, channels)\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        pooled array of shape (batch_size, out_rows, out_cols, channels)\n",
    "        \"\"\"\n",
    "        ### BEGIN YOUR CODE ###\n",
    "        n_examples, in_rows, in_cols, channels = X.shape\n",
    "        pad = self.pad\n",
    "        kernel_height, kernel_width = self.kernel_shape\n",
    "\n",
    "        ### BEGIN YOUR CODE ###\n",
    "        #Padding\n",
    "        if self.pad != (0, 0):\n",
    "            xPad = np.pad(X, ((0,), (pad[0],), (pad[1],), (0,)), constant_values=(0))          \n",
    "        else:\n",
    "            xPad = X\n",
    "\n",
    "        hOut = int(1 + (in_rows + 2*pad[0] - kernel_height) / self.stride)\n",
    "        wOut = int(1 + (in_cols + 2*pad[1] - kernel_width) / self.stride)\n",
    "\n",
    "        #Declaring preactivation output\n",
    "        X_pool = np.zeros((n_examples, hOut, wOut, channels))\n",
    "        indices = np.zeros((hOut, wOut, channels, n_examples), dtype=int)\n",
    "\n",
    "        # implement the forward pass\n",
    "        for hIdx in range(hOut):\n",
    "            for wIdx in range(wOut):\n",
    "                hStart = hIdx * self.stride\n",
    "                hEnd = hIdx * self.stride + kernel_height\n",
    "                wStart = wIdx * self.stride\n",
    "                wEnd = wIdx * self.stride + kernel_width\n",
    "\n",
    "                xWindow = xPad[:, hStart:hEnd, wStart:wEnd, :]\n",
    "\n",
    "                if self.mode == 'max':\n",
    "                    X_pool[:, hIdx, wIdx, :] = self.pool_fn(xWindow, axis = (1, 2))\n",
    "                    \n",
    "                    for c in range(channels):\n",
    "                        flat = xWindow.flatten()\n",
    "                        argmax = np.argmax(np.split(flat, n_examples), axis = 1)\n",
    "                        indices[hIdx, wIdx, c] = argmax\n",
    "\n",
    "                elif self.mode == 'average':\n",
    "                    X_pool[:, hIdx, wIdx, :] = self.pool_fn(xWindow, axis=(1, 2))\n",
    "\n",
    "        # cache any values required for backprop\n",
    "\n",
    "        if self.mode == 'max':\n",
    "            self.cache['indices'] = indices\n",
    "        self.cache['X'] = X\n",
    "\n",
    "        ### END YOUR CODE ###\n",
    "\n",
    "        return X_pool\n",
    "\n",
    "    def backward(self, dLdY: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Backward pass for pooling layer.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        dLdY  gradient of loss with respect to the output of this layer\n",
    "              shape (batch_size, out_rows, out_cols, channels)\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        gradient of loss with respect to the input of this layer\n",
    "        shape (batch_size, in_rows, in_cols, channels)\n",
    "        \"\"\"\n",
    "        ### BEGIN YOUR CODE ###\n",
    "        indices = self.cache['indices']\n",
    "        # print(\"indices: \", indices[0,0])\n",
    "        X = self.cache['X']\n",
    "        batch_size, out_rows, out_cols, channels = dLdY.shape\n",
    "        dX = np.zeros_like(X)\n",
    "        hOut = dLdY.shape[1]\n",
    "        wOut = dLdY.shape[2]\n",
    "        kernel_height, kernel_width = self.kernel_shape\n",
    "        pad = self.pad\n",
    "\n",
    "        if self.pad != (0, 0):\n",
    "            xPad = np.pad(X, ((0,), (pad[0],), (pad[1],), (0,)), constant_values=(0))          \n",
    "        else:\n",
    "            xPad = X\n",
    "\n",
    "        for hIdx in range(hOut):\n",
    "            for wIdx in range(wOut):\n",
    "                for c in range(channels):\n",
    "                    hStart = hIdx * self.stride\n",
    "                    hEnd = hIdx * self.stride + kernel_height\n",
    "                    wStart = wIdx * self.stride\n",
    "                    wEnd = wIdx * self.stride + kernel_width\n",
    "                    xWindow = xPad[:, hStart:hEnd, wStart:wEnd, c]\n",
    "                if self.mode == \"max\":\n",
    "                    maxIdx = np.unravel_index(indices[hIdx, wIdx, c, :], (kernel_height, kernel_width))\n",
    "                    \n",
    "                    dX[:, hStart:hEnd, wStart:wEnd, c] += dLdY[:, hIdx, wIdx, c]\n",
    "\n",
    "                elif self.mode == \"average\":\n",
    "                    dy = dLdY[:, hIdx, wIdx, c]\n",
    "                    dX[:, hStart:hEnd, wStart:wEnd, c] += self.averageMatrix(dy)\n",
    "                    \n",
    "        ### END YOUR CODE ###\n",
    "        return dX\n",
    "    \n",
    "    def averageMatrix(self, dLdY):\n",
    "        kernel_height, kernel_width = self.kernel_shape\n",
    "        average = dLdY / (kernel_height * kernel_width)\n",
    "        return np.ones(self.kernel_shape) * average\n",
    "\n",
    "```\n",
    "\n",
    "Implementation of `layers.Conv2D.__init__`:\n",
    "\n",
    "```python\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_out: int,\n",
    "        kernel_shape: Tuple[int, int],\n",
    "        activation: str,\n",
    "        stride: int = 1,\n",
    "        pad: str = \"same\",\n",
    "        weight_init: str = \"xavier_uniform\",\n",
    "    ) -> None:\n",
    "\n",
    "        super().__init__()\n",
    "        self.n_in = None\n",
    "        self.n_out = n_out\n",
    "        self.kernel_shape = kernel_shape\n",
    "        self.stride = stride\n",
    "        self.pad = pad\n",
    "\n",
    "        self.activation = initialize_activation(activation)\n",
    "        self.init_weights = initialize_weights(weight_init, activation=activation)\n",
    "\n",
    "```\n",
    "\n",
    "Implementation of `layers.Conv2D._init_parameters`:\n",
    "\n",
    "```python\n",
    "    def _init_parameters(self, X_shape: Tuple[int, int, int, int]) -> None:\n",
    "        \"\"\"Initialize all layer parameters and determine padding.\"\"\"\n",
    "        self.n_in = X_shape[3]\n",
    "\n",
    "        W_shape = self.kernel_shape + (self.n_in,) + (self.n_out,)\n",
    "        W = self.init_weights(W_shape)\n",
    "        b = np.zeros((1, self.n_out))\n",
    "\n",
    "        self.parameters = OrderedDict({\"W\": W, \"b\": b}) # DO NOT CHANGE THE KEYS\n",
    "        self.cache = OrderedDict({\"Z\": [], \"X\": []}) # cache for backprop\n",
    "        self.gradients = OrderedDict({\"W\": np.zeros_like(W), \"b\": np.zeros_like(b)}) # parameter gradients initialized to zero\n",
    "                                                                                     # MUST HAVE THE SAME KEYS AS `self.parameters`\n",
    "\n",
    "        if self.pad == \"same\":\n",
    "            self.pad = ((W_shape[0] - 1) // 2, (W_shape[1] - 1) // 2)\n",
    "        elif self.pad == \"valid\":\n",
    "            self.pad = (0, 0)\n",
    "        elif isinstance(self.pad, int):\n",
    "            self.pad = (self.pad, self.pad)\n",
    "        else:\n",
    "            raise ValueError(\"Invalid Pad mode found in self.pad.\")\n",
    "\n",
    "```\n",
    "\n",
    "Implementation of `layers.Conv2D.forward`:\n",
    "\n",
    "```python\n",
    "    def forward(self, X: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Forward pass for convolutional layer. This layer convolves the input\n",
    "        `X` with a filter of weights, adds a bias term, and applies an activation\n",
    "        function to compute the output. This layer also supports padding and\n",
    "        integer strides. Intermediates necessary for the backward pass are stored\n",
    "        in the cache.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X  input with shape (batch_size, in_rows, in_cols, in_channels)\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        output feature maps with shape (batch_size, out_rows, out_cols, out_channels)\n",
    "        \"\"\"\n",
    "        if self.n_in is None:\n",
    "            self._init_parameters(X.shape)\n",
    "\n",
    "        W = self.parameters[\"W\"]\n",
    "        b = self.parameters[\"b\"]\n",
    "\n",
    "        kernel_height, kernel_width, in_channels, out_channels = W.shape\n",
    "        n_examples, in_rows, in_cols, in_channels = X.shape\n",
    "        kernel_shape = (kernel_height, kernel_width)\n",
    "        pad = self.pad\n",
    "\n",
    "        ### BEGIN YOUR CODE ###\n",
    "        #Padding\n",
    "        if self.pad != (0, 0):\n",
    "            xPad = np.pad(X, ((0,), (pad[0],), (pad[1],), (0,)), constant_values=(0))          \n",
    "        else:\n",
    "            xPad = X\n",
    "\n",
    "        hOut = int(1 + (in_rows + 2*pad[0] - kernel_height) / self.stride)\n",
    "        wOut = int(1 + (in_cols + 2*pad[1] - kernel_width) / self.stride)\n",
    "\n",
    "        #Declaring preactivation output\n",
    "        Z = np.empty((n_examples, hOut, wOut, out_channels))\n",
    "\n",
    "        # implement a convolutional forward pass\n",
    "        for hIdx in range(hOut):\n",
    "            for wIdx in range(wOut):\n",
    "                window = xPad[:, hIdx*self.stride : hIdx*self.stride + kernel_height,\n",
    "                              wIdx*self.stride : wIdx*self.stride + kernel_width, :]\n",
    "                for c in range(out_channels):\n",
    "                    Z[:, hIdx, wIdx, c] = np.sum(window*W[:, :, :, c], axis=(1,2,3)) + b[0, c]\n",
    "                # Z[:, wIdx, hIdx, :] = np.einsum('...whc, hwcd -> ...d', window, W) + b[0]\n",
    "\n",
    "        # cache any values required for backprop\n",
    "        self.cache[\"X\"] = X\n",
    "        self.cache[\"Z\"] = Z\n",
    "\n",
    "        #Apply activation\n",
    "        out = self.activation(Z)\n",
    "\n",
    "        # assert out.shape == (n_examples, wOut, hOut, out_channels), \"Wrong output shape\"\n",
    "        ### END YOUR CODE ###\n",
    "\n",
    "        return out\n",
    "\n",
    "```\n",
    "\n",
    "Implementation of `layers.Conv2D.backward`:\n",
    "\n",
    "```python\n",
    "    def backward(self, dLdY: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Backward pass for conv layer. Computes the gradients of the output\n",
    "        with respect to the input feature maps as well as the filter weights and\n",
    "        biases.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        dLdY  gradient of loss with respect to output of this layer\n",
    "              shape (batch_size, out_rows, out_cols, out_channels)\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        gradient of the loss with respect to the input of this layer\n",
    "        shape (batch_size, in_rows, in_cols, in_channels)\n",
    "        \"\"\"\n",
    "        ### BEGIN YOUR CODE ###\n",
    "        X = self.cache[\"X\"]\n",
    "        Z = self.cache[\"Z\"]\n",
    "\n",
    "        W = self.parameters[\"W\"]\n",
    "        b = self.parameters[\"b\"]\n",
    "\n",
    "        kernel_height, kernel_width, in_channels, out_channels = W.shape\n",
    "        n_examples, in_rows, in_cols, in_channels = X.shape\n",
    "        kernel_shape = (kernel_height, kernel_width)\n",
    "        pad = self.pad\n",
    "\n",
    "        ### BEGIN YOUR CODE ###\n",
    "        #Padding\n",
    "        if self.pad != (0, 0):\n",
    "            xPad = np.pad(X, ((0,), (pad[0],), (pad[1],), (0,)), constant_values=(0))          \n",
    "        else:\n",
    "            xPad = X\n",
    "\n",
    "        hOut = dLdY.shape[1]\n",
    "        wOut = dLdY.shape[2]\n",
    "        padH, padW = self.pad\n",
    "\n",
    "        #padding X\n",
    "        if self.pad != (0, 0):\n",
    "            xPad = np.pad(X, ((0,), (pad[0],), (pad[1],), (0,)), constant_values=(0))          \n",
    "        else:\n",
    "            xPad = X\n",
    "        \n",
    "        #Gradients\n",
    "        paddedDx = np.zeros(xPad.shape)\n",
    "        dW = np.zeros(W.shape)\n",
    "        dLdZ = self.activation.backward(Z, dLdY)\n",
    "\n",
    "        # perform a backward pass\n",
    "        for hIdx in range(hOut):\n",
    "            for wIdx in range(wOut):\n",
    "                hStart = hIdx * self.stride\n",
    "                hEnd = hIdx * self.stride + kernel_height\n",
    "                wStart = wIdx * self.stride\n",
    "                wEnd = wIdx * self.stride + kernel_width\n",
    "\n",
    "                xWindow = xPad[:, hStart:hEnd, wStart:wEnd, :]\n",
    "                paddedDx[:, hStart:hEnd, wStart:wEnd, :] += np.einsum('hwio, bo -> bhwi', W, dLdZ[:, hIdx, wIdx, :])\n",
    "                \n",
    "                for c in range(out_channels):\n",
    "                    dW[:,:,:,c] += (xWindow * dLdY[:,hIdx,wIdx,c][:,None,None,None]).sum(axis=0)\n",
    "                   \n",
    "\n",
    "                \n",
    "                \n",
    "                # paddedDx[:, hIdx * self.stride : hIdx * self.stride + kernel_height,\n",
    "                #         wIdx * self.stride : wIdx * self.stride + kernel_width, :] += \\\n",
    "                # 1 \n",
    "                        \n",
    "        dX = paddedDx[:,pad[0]:-pad[0], pad[1]:-pad[1], :]\n",
    "        db = dLdY.sum(axis=(0, 1, 2)).reshape(1, -1)\n",
    "        \n",
    "        self.gradients[\"W\"] = dW\n",
    "        self.gradients[\"b\"] = db\n",
    "\n",
    "        ### END YOUR CODE ###\n",
    "\n",
    "        return dX\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "### Loss Function Implementations:\n",
    "\n",
    "Implementation of `losses.CrossEntropy`:\n",
    "\n",
    "```python\n",
    "class CrossEntropy(Loss):\n",
    "    \"\"\"Cross entropy loss function.\"\"\"\n",
    "\n",
    "    def __init__(self, name: str) -> None:\n",
    "        self.name = name\n",
    "\n",
    "    def __call__(self, Y: np.ndarray, Y_hat: np.ndarray) -> float:\n",
    "        return self.forward(Y, Y_hat)\n",
    "\n",
    "    def forward(self, Y: np.ndarray, Y_hat: np.ndarray) -> float:\n",
    "        \"\"\"Computes the loss for predictions `Y_hat` given one-hot encoded labels\n",
    "        `Y`.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        Y      one-hot encoded labels of shape (batch_size, num_classes)\n",
    "        Y_hat  model predictions in range (0, 1) of shape (batch_size, num_classes)\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        a single float representing the loss\n",
    "        \"\"\"\n",
    "        ### YOUR CODE HERE ###\n",
    "        m = Y.shape[0]\n",
    "        loss = np.sum(np.diag(Y @ np.log(Y_hat).T))\n",
    "        return (-1/m) * loss\n",
    "\n",
    "    def backward(self, Y: np.ndarray, Y_hat: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Backward pass of cross-entropy loss.\n",
    "        NOTE: This is correct ONLY when the loss function is SoftMax.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        Y      one-hot encoded labels of shape (batch_size, num_classes)\n",
    "        Y_hat  model predictions in range (0, 1) of shape (batch_size, num_classes)\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        the gradient of the cross-entropy loss with respect to the vector of\n",
    "        predictions, `Y_hat`\n",
    "        \"\"\"\n",
    "        ### YOUR CODE HERE ###\n",
    "        m = Y.shape[0]\n",
    "        return (-1/m) * (np.divide(Y,Y_hat))\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "### Model Implementations:\n",
    "\n",
    "Implementation of `models.NeuralNetwork.forward`:\n",
    "\n",
    "```python\n",
    "    def forward(self, X: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"One forward pass through all the layers of the neural network.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X  design matrix whose must match the input shape required by the\n",
    "           first layer\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        forward pass output, matches the shape of the output of the last layer\n",
    "        \"\"\"\n",
    "        ### YOUR CODE HERE ###\n",
    "        # Iterate through the network's layers.\n",
    "        output = X\n",
    "\n",
    "        for layer in self.layers:\n",
    "            output = layer.forward(output)\n",
    "        return output\n",
    "\n",
    "```\n",
    "\n",
    "Implementation of `models.NeuralNetwork.backward`:\n",
    "\n",
    "```python\n",
    "    def backward(self, target: np.ndarray, out: np.ndarray) -> float:\n",
    "        \"\"\"One backward pass through all the layers of the neural network.\n",
    "        During this phase we calculate the gradients of the loss with respect to\n",
    "        each of the parameters of the entire neural network. Most of the heavy\n",
    "        lifting is done by the `backward` methods of the layers, so this method\n",
    "        should be relatively simple. Also make sure to compute the loss in this\n",
    "        method and NOT in `self.forward`.\n",
    "\n",
    "        Note: Both input arrays have the same shape.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        target  the targets we are trying to fit to (e.g., training labels)\n",
    "        out     the predictions of the model on training data\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        the loss of the model given the training inputs and targets\n",
    "        \"\"\"\n",
    "        ### YOUR CODE HERE ###\n",
    "        # Compute the loss.\n",
    "        # Backpropagate through the network's layers.\n",
    "        loss = self.loss(target, out)\n",
    "\n",
    "        dLdOut = self.loss.backward(target, out)\n",
    "        for layer in self.layers[::-1]: #Layers in reverse\n",
    "            dLdOut = layer.backward(dLdOut)\n",
    "\n",
    "        return loss\n",
    "\n",
    "```\n",
    "\n",
    "Implementation of `models.NeuralNetwork.predict`:\n",
    "\n",
    "```python\n",
    "    def predict(self, X: np.ndarray, Y: np.ndarray) -> Tuple[np.ndarray, float]:\n",
    "        \"\"\"Make a forward and backward pass to calculate the predictions and\n",
    "        loss of the neural network on the given data.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X  input features\n",
    "        Y  targets (same length as `X`)\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        a tuple of the prediction and loss\n",
    "        \"\"\"\n",
    "        ### YOUR CODE HERE ###\n",
    "        # Do a forward pass. Maybe use a function you already wrote?\n",
    "        # Get the loss. Remember that the `backward` function returns the loss.\n",
    "        Yh = self.forward(X)\n",
    "        L = self.backward(Y, Yh)\n",
    "\n",
    "        return Yh, L\n",
    "\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
