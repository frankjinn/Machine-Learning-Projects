{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sys\n",
    "# !conda install --yes --prefix {sys.prefix} pydot\n",
    "# !conda install --yes --prefix {sys.prefix} graphviz\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "import numpy as np\n",
    "from numpy import genfromtxt\n",
    "import scipy.io\n",
    "from scipy import stats\n",
    "from sklearn.tree import DecisionTreeClassifier, export_graphviz\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from pydot import graph_from_dot_data\n",
    "import io\n",
    "\n",
    "import random\n",
    "random.seed(246810)\n",
    "np.random.seed(246810)\n",
    "\n",
    "eps = 1e-5  # a small number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecisionTree:\n",
    "\n",
    "    def __init__(self, max_depth=3, feature_labels=None, randomForest = False, maxFeatures = None):\n",
    "        self.max_depth = max_depth\n",
    "        self.maxFeatures = maxFeatures\n",
    "        self.randomForest = randomForest\n",
    "        self.features = feature_labels\n",
    "        self.left, self.right = None, None  # for non-leaf nodes\n",
    "        self.split_idx, self.thresh = None, None  # for non-leaf nodes\n",
    "        self.data, self.pred = None, None  # for leaf nodes\n",
    "        self.labels = None # for leaf nodes\n",
    "        self.pure = False # for leaf nodes\n",
    "        self.unsplittable = False # for leaf nodes\n",
    "\n",
    "    @staticmethod\n",
    "    def entropy(y):\n",
    "        \"\"\" Given a set of labels, find the entropy within.\"\"\"\n",
    "        unique, counts = np.unique(y, return_counts=True)\n",
    "        freq = dict(zip(unique, counts))\n",
    "        result = 0\n",
    "        for c in freq.keys():\n",
    "            pc = freq[c]/len(y)\n",
    "            result += pc * np.log2(pc)\n",
    "        return -result\n",
    "\n",
    "    def information_gain(self, X, y, idx, thresh):\n",
    "        \"\"\" Given a feature index (idx), and the threshold, we split\n",
    "            and find the information gain from that split.\"\"\"\n",
    "        \n",
    "        X0, y0, X1, y1 = self.split(X, y, idx, thresh)\n",
    "        Hs = DecisionTree.entropy(y)\n",
    "        Hafter = (len(y0)*DecisionTree.entropy(y0) + len(y1)*DecisionTree.entropy(y1))\n",
    "        Hafter = Hafter/(len(y0) + len(y1))\n",
    "        return Hs - Hafter\n",
    "\n",
    "    @staticmethod\n",
    "    def gini_impurity(y):\n",
    "        #Calculate impurity for a set of rows\n",
    "        impurity = 1\n",
    "        unique, counts = np.unique(y, return_counts=True)\n",
    "        freq = dict(zip(unique, counts))\n",
    "        for label in freq:\n",
    "            pc = freq[label]/len(y)\n",
    "            impurity -= pc**2\n",
    "        return impurity\n",
    "    \n",
    "    def gini_purification(self, X, y, idx, thresh):\n",
    "        X0, y0, X1, y1 = self.split(X, y, idx, thresh)\n",
    "        p = float(len(y0)) / (len(y0) + len(y1))\n",
    "        return DecisionTree.gini_impurity(y) - p * DecisionTree.gini_impurity(y0) - (1 - p) * DecisionTree.gini_impurity(y1)\n",
    "\n",
    "    def split(self, X, y, idx, thresh):\n",
    "        X0, idx0, X1, idx1 = self.split_test(X, idx=idx, thresh=thresh)\n",
    "        y0, y1 = y[idx0], y[idx1]\n",
    "        return X0, y0, X1, y1\n",
    "\n",
    "    def split_test(self, X, idx, thresh):\n",
    "        idx0 = np.where(X[:, idx] < thresh)[0]\n",
    "        idx1 = np.where(X[:, idx] >= thresh)[0]\n",
    "        X0, X1 = X[idx0, :], X[idx1, :]\n",
    "        return X0, idx0, X1, idx1\n",
    "\n",
    "    def fit(self, X, y, gain = 'gini'):\n",
    "        \"\"\" Find the feature and threshold that gives the best increase in\n",
    "            information gain. Split, then recursively call on children. Currently uses entropy.\"\"\"\n",
    "\n",
    "        #First check if y is all in 1 class\n",
    "        if np.all(y == y[0]):\n",
    "            self.data, self.labels, self.pred = X, y, y[0]\n",
    "            self.pure = True\n",
    "            return self\n",
    "        \n",
    "        #If they have same labels but different classes, take most likely class\n",
    "        if np.all(X == X[0]) or self.max_depth == 0:\n",
    "            mostLikely, _ = stats.mode(y)\n",
    "            self.data, self.labels, self.pred = X, y, mostLikely\n",
    "            if np.all(X == X[0]):\n",
    "                self.unsplittable = True\n",
    "            return self\n",
    "\n",
    "        #If doing random forest, take random sample of X.\n",
    "        if self.randomForest:\n",
    "            xTrue = X\n",
    "            #Select random sample of features\n",
    "            idx = np.random.choice(X.shape[1], size=self.maxFeatures, replace=False)\n",
    "            X = np.array([X[i][idx] for i in range(X.shape[0])])\n",
    "\n",
    "        bestGain = 0\n",
    "        bestIdx = None\n",
    "        bestThresh = None\n",
    "        nFeatures = len(X[0])\n",
    "\n",
    "        for featureIdx in range(nFeatures):\n",
    "            vals = set([sample[featureIdx] for sample in X])\n",
    "\n",
    "            for thresh in vals:\n",
    "                if gain == 'entropy':\n",
    "                    currGain = self.information_gain(X, y, featureIdx, thresh)\n",
    "                elif gain == 'gini':\n",
    "                    currGain = self.gini_purification(X, y, featureIdx, thresh)\n",
    "                \n",
    "                X0, y0, X1, y1 = self.split(X, y, featureIdx, thresh)\n",
    "\n",
    "                #If the current made a empty side\n",
    "                if len(y0) == 0 or len(y1) == 0:\n",
    "                    continue\n",
    "            \n",
    "                if currGain >= bestGain:\n",
    "                    bestIdx = featureIdx\n",
    "                    bestThresh = thresh\n",
    "                    bestGain = currGain\n",
    "        \n",
    "        #Split on best index and feature\n",
    "        self.split_idx = bestIdx\n",
    "        self.thresh = bestThresh\n",
    "        \n",
    "        if self.thresh == None:\n",
    "            print(self.split_idx, self.thresh)\n",
    "            print(\"Best Gain: \", bestGain)\n",
    "            print(\"X: \\n\", X)\n",
    "            print(\"Y: \\n\", y)\n",
    "\n",
    "        #If splitting using random forest, restore old X.\n",
    "        if self.randomForest:\n",
    "            X = xTrue\n",
    "        \n",
    "        X0, y0, X1, y1 = self.split(X, y, self.split_idx, self.thresh)\n",
    "\n",
    "        #After figuring out this data, we can now set it for this treenode\n",
    "        self.left = DecisionTree(max_depth = self.max_depth - 1, feature_labels=self.features)\n",
    "        self.left = self.left.fit(X0, y0)\n",
    "        self.right = DecisionTree(max_depth = self.max_depth - 1, feature_labels=self.features)\n",
    "        self.right = self.right.fit(X1, y1)\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        if self.pred != None:\n",
    "            return self.pred\n",
    "        \n",
    "        # print(self.split_idx)\n",
    "        # print(X)\n",
    "        # print(X[self.split_idx])\n",
    "        \n",
    "        if X[self.split_idx] < self.thresh:\n",
    "            return self.left.predict(X)\n",
    "        else:\n",
    "            return self.right.predict(X)\n",
    "        \n",
    "    # def predictSet(self, X):\n",
    "    #     preds = []\n",
    "    #     for sample in X:\n",
    "    #         preds.append(self.predictSample(sample))\n",
    "    #     return preds\n",
    "        \n",
    "    # def predictSample(self, X):\n",
    "    #     if self.pred != None:\n",
    "    #         return self.pred\n",
    "        \n",
    "    #     if X[self.split_idx] < self.thresh:\n",
    "    #         return self.left.predictSample(X)\n",
    "    #     else:\n",
    "    #         return self.right.predictSample(X)\n",
    "        \n",
    "    def __repr__(self):\n",
    "        if self.max_depth == 0 or self.pure == True or self.unsplittable == True:\n",
    "            return \"%s (%s)\" % (self.pred, self.data.size)\n",
    "        else:\n",
    "            return \"[%s < %s: %s | %s]\" % (self.features[self.split_idx],\n",
    "                                           self.thresh, self.left.__repr__(),\n",
    "                                           self.right.__repr__())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3 0 6 1]]\n"
     ]
    }
   ],
   "source": [
    "a = np.array([[3, 0, 3, 7],\n",
    "              [3, 2, 6, 2],\n",
    "              [1, 7, 2, 8],\n",
    "              [3, 0, 6, 1],\n",
    "              [3, 2, 5, 5]])\n",
    "mode, _ = stats.mode(a, keepdims=True)\n",
    "print(mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(data, fill_mode=True, min_freq=10, onehot_cols=[]):\n",
    "    # fill_mode = False\n",
    "    \n",
    "    # Temporarily assign -1 to missing data\n",
    "    data[data == b''] = '-1'\n",
    "\n",
    "    # Hash the columns (used for handling strings)\n",
    "    onehot_encoding = []\n",
    "    onehot_features = []\n",
    "    for col in onehot_cols:\n",
    "        counter = Counter(data[:, col])\n",
    "        for term in counter.most_common():\n",
    "            if term[0] == b'-1':\n",
    "                continue\n",
    "            if term[-1] <= min_freq:\n",
    "                break\n",
    "            onehot_features.append(term[0])\n",
    "            onehot_encoding.append((data[:, col] == term[0]).astype(float))\n",
    "        data[:, col] = '0'\n",
    "    onehot_encoding = np.array(onehot_encoding).T\n",
    "    data = np.hstack(\n",
    "        [np.array(data, dtype=float),\n",
    "         np.array(onehot_encoding)])\n",
    "\n",
    "    # print(\"OneHot Features:\\n\", onehot_features)\n",
    "    # print(\"OneHot Encoding:\\n\", onehot_encoding)\n",
    "    # print(\"Data after -1 (First 10):\\n\", data[:10])\n",
    "    # print(\"Data shape: \", data.shape)\n",
    "\n",
    "    # Replace missing data with the mode value. We use the mode instead of\n",
    "    # the mean or median because this makes more sense for categorical\n",
    "    # features such as gender or cabin type, which are not ordered.\n",
    "    if fill_mode:\n",
    "        modeList = []\n",
    "        # print(data[:, 13].shape)\n",
    "        for col in range(len(data[0])):\n",
    "            column = [val for val in data[:, col] if val != -1]\n",
    "            # print(column)\n",
    "            mode, _ = stats.mode(column, keepdims=False)\n",
    "            modeList.append(mode)\n",
    "\n",
    "        # print(modeList)\n",
    "        noDataIdx = list(zip(*np.where(data == -1)))\n",
    "        # print(noDataIdx)\n",
    "        for (x, y) in noDataIdx:\n",
    "            # print(x, y, modeList[y])\n",
    "            data[x][y] = modeList[y]\n",
    "\n",
    "        \n",
    "        # print(\"Data after fill: \\n\", data[:10])\n",
    "        \n",
    "\n",
    "    # print(\"Data: \", data)\n",
    "    # print(\"One hot: \",onehot_features)\n",
    "    return data, onehot_features\n",
    "\n",
    "\n",
    "def evaluate(clf):\n",
    "    print(\"Cross validation\", cross_val_score(clf, X, y))\n",
    "    if hasattr(clf, \"decision_trees\"):\n",
    "        counter = Counter([t.tree_.feature[0] for t in clf.decision_trees])\n",
    "        first_splits = [\n",
    "            (features[term[0]], term[1]) for term in counter.most_common()\n",
    "        ]\n",
    "        print(\"First splits\", first_splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train/Val size (4503, 32) (1126, 32)\n",
      "Features ['pain', 'private', 'bank', 'money', 'drug', 'spam', 'prescription', 'creative', 'height', 'featured', 'differ', 'width', 'other', 'energy', 'business', 'message', 'volumes', 'revision', 'path', 'meter', 'memo', 'planning', 'pleased', 'record', 'out', 'semicolon', 'dollar', 'sharp', 'exclamation', 'parenthesis', 'square_bracket', 'ampersand']\n",
      "Train/test size (5629, 32) (5400, 32)\n"
     ]
    }
   ],
   "source": [
    "# dataset = \"titanic\"\n",
    "dataset = \"spam\"\n",
    "params = {\n",
    "    \"max_depth\": 5,\n",
    "    # \"random_state\": 6,\n",
    "    \"min_samples_leaf\": 10,\n",
    "}\n",
    "N = 100\n",
    "\n",
    "if dataset == \"titanic\":\n",
    "    # Load titanic data\n",
    "    path_train = 'datasets/titanic/titanic_training.csv'\n",
    "    data = genfromtxt(path_train, delimiter=',', dtype=None)\n",
    "    path_test = 'datasets/titanic/titanic_testing_data.csv'\n",
    "    test_data = genfromtxt(path_test, delimiter=',', dtype=None)\n",
    "    y = data[1:, 0]  # label = survived\n",
    "    class_names = [\"Died\", \"Survived\"]\n",
    "\n",
    "    # print(data.shape)\n",
    "    # print(data[:, 1:][0:10])\n",
    "\n",
    "    labeled_idx = np.where(y != b'')[0]\n",
    "    y = np.array(y[labeled_idx], dtype=float).astype(int)\n",
    "    print(\"\\n\\nPart (b): preprocessing the titanic dataset\")\n",
    "    X, onehot_features = preprocess(data[1:, 1:], onehot_cols=[1, 5, 7, 8])\n",
    "    X = X[labeled_idx, :]\n",
    "\n",
    "    #Split into training and validation set\n",
    "    xTrain, xVal, yTrain, yVal = train_test_split(X, y, test_size=0.2, random_state=88, shuffle=True)\n",
    "    print(\"Train/Val size\", xTrain.shape, xVal.shape)\n",
    "\n",
    "    Z, _ = preprocess(test_data[1:, :], onehot_cols=[1, 5, 7, 8])\n",
    "    assert X.shape[1] == Z.shape[1]\n",
    "    features = list(data[0, 1:]) + onehot_features\n",
    "    # print(features)\n",
    "elif dataset == \"spam\":\n",
    "    features = [\n",
    "        \"pain\", \"private\", \"bank\", \"money\", \"drug\", \"spam\", \"prescription\",\n",
    "        \"creative\", \"height\", \"featured\", \"differ\", \"width\", \"other\",\n",
    "        \"energy\", \"business\", \"message\", \"volumes\", \"revision\", \"path\",\n",
    "        \"meter\", \"memo\", \"planning\", \"pleased\", \"record\", \"out\",\n",
    "        \"semicolon\", \"dollar\", \"sharp\", \"exclamation\", \"parenthesis\",\n",
    "        \"square_bracket\", \"ampersand\"\n",
    "    ]\n",
    "    assert len(features) == 32\n",
    "\n",
    "    # Load spam data\n",
    "    path_train = 'datasets/spam_data/spam_data.mat'\n",
    "    data = scipy.io.loadmat(path_train)\n",
    "    X = data['training_data']\n",
    "    y = np.squeeze(data['training_labels'])\n",
    "    Z = data['test_data']\n",
    "    class_names = [\"Ham\", \"Spam\"]\n",
    "\n",
    "    #Split into training and validation set\n",
    "    xTrain, xVal, yTrain, yVal = train_test_split(X, y, test_size=0.2, random_state=88, shuffle=True)\n",
    "    print(\"Train/Val size\", xTrain.shape, xVal.shape)\n",
    "    \n",
    "else:\n",
    "    raise NotImplementedError(\"Dataset %s not handled\" % dataset)\n",
    "\n",
    "print(\"Features\", features)\n",
    "print(\"Train/test size\", X.shape, Z.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Part 0: constant classifier\n",
      "Accuracy 0.7258838159531\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\\nPart 0: constant classifier\")\n",
    "print(\"Accuracy\", 1 - np.sum(y) / y.size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(preds):\n",
    "    result = 0\n",
    "    for idx in range(len(preds)):\n",
    "        if preds[idx] == yVal[idx]:\n",
    "            result += 1\n",
    "    result = result / len(preds)\n",
    "    return result\n",
    "def validate(model):\n",
    "    preds = []\n",
    "    for sample in xVal:\n",
    "        preds.append(model.predict(sample))\n",
    "    return accuracy(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[exclamation < 1.0: [parenthesis < 1.0: [creative < 1.0: [money < 1.0: [pain < 1.0: [meter < 1.0: [dollar < 1.0: [prescription < 1.0: [ampersand < 1.0: [spam < 1.0: [private < 1.0: [other < 1.0: 0.0 (37056) | 0.0 (2528)] | [business < 1.0: 1.0 (384) | 0.0 (64)]] | 1.0 (160)] | [out < 2.0: [semicolon < 1.0: [message < 1.0: 0.0 (3648) | 0.0 (160)] | [other < 1.0: 0.0 (160) | 1.0 (32)]] | [square_bracket < 1.0: 0.0 (128) | 1.0 (32)]]] | 1.0 (256)] | [energy < 1.0: [volumes < 1.0: [dollar < 2.0: [ampersand < 1.0: [sharp < 1.0: 1.0 (1536) | 0.0 (96)] | 0.0 (96)] | [dollar < 54.0: [dollar < 9.0: 1.0 (1472) | 1.0 (160)] | 0.0 (64)]] | 0.0 (192)] | 0.0 (448)]] | 0.0 (4800)] | [pain < 2.0: [out < 1.0: [message < 1.0: [other < 1.0: [differ < 1.0: 1.0 (288) | 1.0 (32)] | 1.0 (32)] | 1.0 (32)] | 1.0 (32)] | 1.0 (96)]] | [business < 1.0: [energy < 1.0: [semicolon < 2.0: [sharp < 1.0: [semicolon < 1.0: [out < 1.0: [message < 1.0: [dollar < 1.0: 0.0 (224) | 0.0 (256)] | 1.0 (32)] | 1.0 (64)] | 1.0 (64)] | 1.0 (64)] | 0.0 (64)] | 0.0 (96)] | [ampersand < 2.0: 1.0 (672) | 0.0 (32)]]] | [semicolon < 1.0: 1.0 (864) | [semicolon < 3.0: 0.0 (32) | 1.0 (96)]]] | [money < 1.0: [prescription < 1.0: [featured < 1.0: [dollar < 2.0: [message < 1.0: [spam < 1.0: [bank < 1.0: [energy < 1.0: [differ < 1.0: [parenthesis < 2.0: 0.0 (15008) | 0.0 (13120)] | [parenthesis < 5.0: 1.0 (32) | 0.0 (64)]] | 0.0 (4672)] | [parenthesis < 2.0: [square_bracket < 1.0: [semicolon < 1.0: 0.0 (128) | 1.0 (32)] | 1.0 (32)] | [business < 1.0: 0.0 (224) | [parenthesis < 3.0: 1.0 (32) | 0.0 (64)]]]] | 1.0 (32)] | [parenthesis < 4.0: [ampersand < 1.0: [sharp < 4.0: [out < 3.0: [business < 6.0: 0.0 (4160) | 1.0 (32)] | 1.0 (32)] | 1.0 (32)] | 0.0 (800)] | [differ < 1.0: [ampersand < 3.0: 0.0 (2112) | [message < 4.0: 0.0 (160) | 1.0 (32)]] | 1.0 (32)]]] | [energy < 1.0: [parenthesis < 11.0: [square_bracket < 4.0: [ampersand < 1.0: [volumes < 1.0: [other < 1.0: 0.0 (896) | 0.0 (480)] | 0.0 (192)] | [out < 2.0: 0.0 (448) | [dollar < 6.0: 0.0 (96) | 1.0 (32)]]] | 1.0 (64)] | 1.0 (96)] | 0.0 (832)]] | [ampersand < 1.0: 1.0 (128) | 0.0 (32)]] | 1.0 (128)] | [business < 2.0: [money < 3.0: [square_bracket < 2.0: [dollar < 2.0: [dollar < 1.0: [energy < 4.0: [bank < 2.0: 0.0 (224) | [out < 1.0: 1.0 (32) | 0.0 (32)]] | 1.0 (32)] | [planning < 1.0: 1.0 (96) | 0.0 (32)]] | 0.0 (384)] | 1.0 (32)] | [ampersand < 1.0: 1.0 (256) | [dollar < 2.0: 1.0 (32) | 0.0 (96)]]] | [ampersand < 16.0: 1.0 (384) | 0.0 (32)]]]] | [parenthesis < 1.0: [money < 1.0: [ampersand < 1.0: [energy < 1.0: [dollar < 2.0: [sharp < 2.0: [message < 3.0: [prescription < 1.0: [meter < 1.0: [business < 1.0: [pain < 1.0: 1.0 (12224) | 1.0 (416)] | [message < 1.0: 1.0 (1472) | 0.0 (96)]] | 0.0 (128)] | 1.0 (608)] | 0.0 (160)] | [semicolon < 8.0: [width < 1.0: 0.0 (384) | 1.0 (32)] | 1.0 (64)]] | [square_bracket < 6.0: 1.0 (1088) | 0.0 (32)]] | [exclamation < 3.0: [semicolon < 4.0: [energy < 3.0: [other < 1.0: [energy < 2.0: [exclamation < 2.0: 0.0 (256) | [message < 1.0: 0.0 (160) | 0.0 (64)]] | [exclamation < 2.0: 1.0 (32) | 0.0 (64)]] | [exclamation < 2.0: 1.0 (32) | 0.0 (32)]] | 0.0 (224)] | 1.0 (32)] | [energy < 2.0: [exclamation < 4.0: [sharp < 1.0: [business < 2.0: 0.0 (32) | 1.0 (32)] | 1.0 (32)] | 1.0 (160)] | [exclamation < 6.0: 0.0 (64) | 1.0 (32)]]]] | [exclamation < 7.0: [dollar < 18.0: [message < 1.0: [exclamation < 2.0: [other < 1.0: [sharp < 1.0: [dollar < 1.0: 0.0 (352) | [ampersand < 2.0: 0.0 (64) | 1.0 (32)]] | [ampersand < 2.0: [sharp < 3.0: 1.0 (32) | 0.0 (32)] | 0.0 (32)]] | [volumes < 2.0: 1.0 (32) | 0.0 (32)]] | 0.0 (448)] | [exclamation < 2.0: 0.0 (32) | 1.0 (96)]] | 1.0 (64)] | 1.0 (96)]] | [sharp < 3.0: [dollar < 1.0: 1.0 (2176) | [dollar < 2.0: [exclamation < 2.0: 0.0 (32) | 1.0 (96)] | 1.0 (288)]] | [dollar < 3.0: 0.0 (32) | 1.0 (96)]]] | [dollar < 2.0: [money < 1.0: [featured < 1.0: [parenthesis < 2.0: [drug < 1.0: [business < 1.0: [meter < 1.0: [exclamation < 2.0: [ampersand < 1.0: [semicolon < 2.0: 0.0 (1760) | 0.0 (224)] | 0.0 (224)] | [square_bracket < 1.0: [semicolon < 1.0: 0.0 (1472) | 1.0 (96)] | [square_bracket < 4.0: 1.0 (192) | 0.0 (32)]]] | 0.0 (320)] | 0.0 (512)] | 1.0 (128)] | [spam < 1.0: [differ < 1.0: [record < 1.0: [prescription < 2.0: [ampersand < 1.0: [parenthesis < 3.0: 0.0 (1792) | 0.0 (4352)] | 0.0 (1824)] | 1.0 (32)] | [ampersand < 2.0: 1.0 (64) | 0.0 (32)]] | 1.0 (64)] | 1.0 (96)]] | [energy < 1.0: 1.0 (352) | [ampersand < 1.0: 1.0 (32) | 0.0 (32)]]] | [sharp < 2.0: [square_bracket < 3.0: [record < 2.0: [dollar < 1.0: [business < 1.0: [exclamation < 4.0: 1.0 (384) | [exclamation < 5.0: [ampersand < 1.0: 0.0 (32) | 1.0 (32)] | 1.0 (128)]] | [exclamation < 4.0: 0.0 (32) | 1.0 (64)]] | [parenthesis < 2.0: [square_bracket < 1.0: 0.0 (64) | 1.0 (32)] | 1.0 (96)]] | 0.0 (32)] | 0.0 (32)] | 0.0 (64)]] | [ampersand < 1.0: [parenthesis < 2.0: [energy < 4.0: [record < 2.0: [meter < 1.0: [dollar < 3.0: 1.0 (320) | [other < 2.0: [exclamation < 5.0: [exclamation < 3.0: 1.0 (160) | 0.0 (32)] | 1.0 (160)] | 0.0 (32)]] | 0.0 (32)] | 0.0 (32)] | 0.0 (96)] | [parenthesis < 17.0: [energy < 1.0: [message < 1.0: [dollar < 3.0: [exclamation < 3.0: 1.0 (128) | [semicolon < 1.0: [exclamation < 15.0: 0.0 (64) | 1.0 (32)] | 1.0 (96)]] | [parenthesis < 8.0: 1.0 (1056) | [exclamation < 2.0: 0.0 (32) | 1.0 (96)]]] | 1.0 (1440)] | [parenthesis < 4.0: 0.0 (32) | 1.0 (128)]] | [semicolon < 2.0: 0.0 (64) | 1.0 (32)]]] | [exclamation < 3.0: [parenthesis < 13.0: 0.0 (608) | 1.0 (32)] | [dollar < 12.0: 1.0 (224) | [ampersand < 10.0: 0.0 (32) | 1.0 (32)]]]]]]]"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "giniTree = DecisionTree(max_depth = 12, feature_labels=features)\n",
    "giniTree.fit(xTrain, yTrain, gain='gini')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8303730017761989"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validate(giniTree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[exclamation < 1.0: [parenthesis < 1.0: [creative < 1.0: 0.0 (55552) | 1.0 (992)] | [money < 1.0: 0.0 (44256) | 1.0 (1664)]] | [parenthesis < 1.0: [money < 1.0: 1.0 (19296) | 1.0 (2720)] | [dollar < 2.0: 0.0 (14624) | 1.0 (4992)]]]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entTree = DecisionTree(max_depth = 3, feature_labels=features)\n",
    "entTree.fit(xTrain, yTrain, gain='entropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "0.8001776198934281\n"
     ]
    }
   ],
   "source": [
    "preds = []\n",
    "for sample in xVal:\n",
    "    preds.append(entTree.predict(sample))\n",
    "\n",
    "print(preds)\n",
    "print(accuracy(preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "sklearn's decision tree\n",
      "Cross validation [0.8259325  0.81971581 0.81882771 0.830373   0.80888889]\n"
     ]
    }
   ],
   "source": [
    "# sklearn decision tree\n",
    "print(\"\\n\\nsklearn's decision tree\")\n",
    "clf = DecisionTreeClassifier(random_state=0, **params)\n",
    "clf.fit(X, y)\n",
    "evaluate(clf)\n",
    "out = io.StringIO()\n",
    "export_graphviz(\n",
    "    clf, out_file=out, feature_names=features, class_names=class_names)\n",
    "# For OSX, may need the following for dot: brew install gprof2dot\n",
    "graph = graph_from_dot_data(out.getvalue())\n",
    "graph_from_dot_data(out.getvalue())[0].write_pdf(\"%s-tree.pdf\" % dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaggedTrees:\n",
    "\n",
    "    # def __init__(self, max_depth, feature_labels, subsetSize, numTrees=200):\n",
    "    def __init__(self, params=None, subsetSize=100, numTrees=200):\n",
    "        if params is None:\n",
    "            params = {}\n",
    "        self.params = params\n",
    "        # self.max_depth = max_depth\n",
    "        # self.features= feature_labels\n",
    "        self.n = numTrees\n",
    "        self.subsetSize = subsetSize\n",
    "        self.decision_trees = [\n",
    "            DecisionTree(**self.params)\n",
    "            for i in range(self.n)\n",
    "        ]\n",
    "\n",
    "    def bagFit(self, X, y):\n",
    "        #Make n subsets of data\n",
    "        xSubset = []\n",
    "        ySubset = []\n",
    "        for _ in range(self.n):\n",
    "            idx = np.random.choice(X.shape[0], size=self.subsetSize, replace=True)\n",
    "            xSubset.append(X[idx])\n",
    "            ySubset.append(y[idx])\n",
    "        # print(\"xSubset: \", xSubset[0])\n",
    "        # print(\"ySubset: \", ySubset[0].shape)\n",
    "\n",
    "        for i in range(len(self.decision_trees)):\n",
    "            self.decision_trees[i].fit(xSubset[i], ySubset[i])\n",
    "        return self\n",
    "\n",
    "    def bagPredict(self, X):\n",
    "        preds = []\n",
    "        for i in range(len(self.decision_trees)):\n",
    "            preds.append(self.decision_trees[i].predict(X))\n",
    "        return round(np.average(preds))\n",
    "        \n",
    "\n",
    "class RandomForest(BaggedTrees):\n",
    "\n",
    "    def __init__(self, params=None, maxFeatures=10, subsetSize=100, numTrees=200):\n",
    "        if params is None:\n",
    "            params = {}\n",
    "        params['maxFeatures'] = maxFeatures\n",
    "        self.m = maxFeatures\n",
    "        super().__init__(params, subsetSize, numTrees)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.BaggedTrees at 0x7fddf430b390>"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params = {'max_depth':4, 'feature_labels':features}\n",
    "bagTree = BaggedTrees(params, xTrain.shape[0], 50)\n",
    "bagTree.bagFit(xTrain, yTrain)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8099467140319716\n"
     ]
    }
   ],
   "source": [
    "preds = []\n",
    "for sample in xVal:\n",
    "    preds.append(bagTree.bagPredict(sample))\n",
    "print(accuracy(preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = []\n",
    "for subsetSize in np.arange(90, 110, step=2):\n",
    "    for trees in np.arange(20, 100, step=20):\n",
    "        params = {'max_depth':3, 'feature_labels':features}\n",
    "        bagTree = BaggedTrees(params, subsetSize, trees)\n",
    "        bagTree.bagFit(xTrain, yTrain)\n",
    "        preds = []\n",
    "        for sample in xVal:\n",
    "            preds.append(bagTree.bagPredict(sample))\n",
    "        result.append((subsetSize, trees, accuracy(preds)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.sort(key= lambda x: x[2], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(94, 60, 0.8134991119005328),\n",
       " (106, 80, 0.8126110124333925),\n",
       " (100, 20, 0.8117229129662522),\n",
       " (98, 40, 0.8108348134991119),\n",
       " (108, 80, 0.8099467140319716),\n",
       " (92, 60, 0.8081705150976909),\n",
       " (108, 40, 0.8081705150976909),\n",
       " (92, 20, 0.80550621669627),\n",
       " (100, 60, 0.80550621669627),\n",
       " (104, 60, 0.80550621669627)]"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.RandomForest at 0x7fdddf008ed0>"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params = {'max_depth':3, 'feature_labels':features, 'randomForest':False, 'maxFeatures':np.sqrt(xTrain.shape[1])}\n",
    "randForest = RandomForest(params, subsetSize=98, numTrees=60)\n",
    "randForest.bagFit(xTrain, yTrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8028419182948491\n"
     ]
    }
   ],
   "source": [
    "preds = []\n",
    "for sample in xVal:\n",
    "    preds.append(randForest.bagPredict(sample))\n",
    "print(accuracy(preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
